{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1635\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "path = \"./../dist/char*.txt\"\n",
    "contents=[]\n",
    "for filename in glob.glob(path):\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            contents.append(line.strip('\\n'))\n",
    "print(len(contents))\n",
    "i=0\n",
    "while (i<len(contents)-1):\n",
    "    contents[i]=float(contents[i])\n",
    "    i+=1\n",
    "    if ((i+1)%15==0):\n",
    "        i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint as r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  :  tensor([3.7400e+02, 1.9200e+02, 1.6980e+03, 8.6543e+02, 1.7095e+00, 3.4959e+00,\n",
      "        2.7036e+01, 2.3128e+01, 1.3257e+05, 1.8040e+04, 1.0058e+01, 1.3623e+00,\n",
      "        3.5233e-02, 6.8635e-02])\n",
      "1  :  tensor([1.6800e+02, 1.8000e+02, 6.1423e+02, 6.6076e+02, 1.7579e+00, 1.7579e+00,\n",
      "        2.1776e+01, 2.6145e+01, 3.0122e+05, 2.6802e+04, 1.8465e+01, 1.6489e+00,\n",
      "        7.8435e-02, 7.2972e-02])\n",
      "2  :  tensor([1.6800e+02, 2.1600e+02, 3.5532e+04, 2.3201e+04, 3.4959e+00, 3.3456e+00,\n",
      "        2.4845e+01, 2.1307e+01, 3.2577e+03, 4.9163e+02, 1.1552e+01, 8.8423e-01,\n",
      "        7.8440e-02, 6.1009e-02])\n",
      "3  :  tensor([1.7000e+02, 1.9600e+02, 1.0244e+03, 1.1821e+03, 4.2629e+00, 4.6510e+00,\n",
      "        1.8965e+01, 1.6539e+01, 9.7704e+04, 7.7159e+03, 9.8711e+00, 7.7954e-01,\n",
      "        5.7453e-02, 4.9622e-02])\n",
      "4  :  tensor([1.3700e+02, 2.0300e+02, 8.2433e+02, 1.2244e+03, 4.7254e+00, 4.8075e+00,\n",
      "        1.6646e+01, 1.8780e+01, 9.7918e+04, 5.9891e+03, 9.8917e+00, 6.0503e-01,\n",
      "        6.9876e-02, 4.8330e-02])\n",
      "5  :  tensor([1.2100e+02, 1.8000e+02, 7.2573e+02, 1.0826e+03, 4.9331e+00, 4.3437e+00,\n",
      "        1.5881e+01, 1.8789e+01, 9.7496e+04, 7.2560e+03, 9.8272e+00, 7.3138e-01,\n",
      "        7.9711e-02, 5.4328e-02])\n",
      "6  :  tensor([1.6400e+02, 1.7700e+02, 9.8173e+02, 1.0601e+03, 4.4259e+00, 4.2629e+00,\n",
      "        1.8905e+01, 1.8064e+01, 9.8584e+04, 9.4879e+03, 9.8960e+00, 9.5251e-01,\n",
      "        5.9634e-02, 5.5542e-02])\n",
      "7  :  tensor([1.7300e+02, 2.0700e+02, 2.5806e+03, 3.0908e+03, 7.3482e+00, 7.6665e+00,\n",
      "        1.3045e+01, 1.2749e+01, 3.9487e+04, 2.1258e+03, 9.8743e+00, 5.3158e-01,\n",
      "        2.2595e-02, 1.8879e-02])\n",
      "8  :  tensor([1.0800e+02, 2.3700e+02, 1.6098e+03, 3.5507e+03, 7.5637e+00, 7.3482e+00,\n",
      "        1.2387e+01, 1.2993e+01, 3.9534e+04, 1.9359e+03, 9.9132e+00, 4.8544e-01,\n",
      "        3.6194e-02, 1.6755e-02])\n",
      "9  :  tensor([1.3700e+02, 1.9400e+02, 2.0369e+03, 2.8907e+03, 7.5709e+00, 7.3638e+00,\n",
      "        1.2528e+01, 1.2246e+01, 3.9598e+04, 1.8791e+03, 9.8846e+00, 4.6907e-01,\n",
      "        2.8022e-02, 2.0258e-02])\n",
      "10  :  tensor([1.2300e+02, 1.7200e+02, 1.8043e+03, 2.5290e+03, 7.5709e+00, 7.4676e+00,\n",
      "        1.2826e+01, 1.2732e+01, 4.0463e+04, 1.8161e+03, 9.9736e+00, 4.4764e-01,\n",
      "        3.1537e-02, 2.2552e-02])\n",
      "11  :  tensor([1.3400e+02, 1.8100e+02, 1.9851e+03, 2.6866e+03, 7.5637e+00, 7.4384e+00,\n",
      "        1.2636e+01, 1.2380e+01, 3.9989e+04, 1.8436e+03, 9.9476e+00, 4.5860e-01,\n",
      "        2.9425e-02, 2.1199e-02])\n",
      "12  :  tensor([1.3900e+02, 1.7800e+02, 2.0741e+03, 2.6603e+03, 7.5116e+00, 7.3486e+00,\n",
      "        1.2101e+01, 1.3045e+01, 3.9574e+04, 1.6117e+03, 9.9133e+00, 4.0373e-01,\n",
      "        2.7928e-02, 2.2107e-02])\n",
      "13  :  tensor([1.8100e+02, 2.4100e+02, 2.6980e+03, 3.5973e+03, 7.5709e+00, 7.3734e+00,\n",
      "        1.2489e+01, 1.3028e+01, 3.9466e+04, 1.4060e+03, 9.8591e+00, 3.5124e-01,\n",
      "        2.1436e-02, 1.6448e-02])\n",
      "14  :  tensor([1.2200e+02, 1.9300e+02, 1.8168e+03, 2.8829e+03, 7.4135e+00, 7.3482e+00,\n",
      "        1.3028e+01, 1.2820e+01, 3.9861e+04, 1.7196e+03, 9.9751e+00, 4.3034e-01,\n",
      "        3.1246e-02, 2.0363e-02])\n",
      "15  :  tensor([1.2300e+02, 2.2400e+02, 1.8213e+03, 3.3292e+03, 7.3857e+00, 7.4384e+00,\n",
      "        1.2875e+01, 1.3000e+01, 4.0177e+04, 1.5724e+03, 9.9968e+00, 3.9124e-01,\n",
      "        3.1512e-02, 1.7567e-02])\n",
      "16  :  tensor([1.0000e+02, 1.8300e+02, 1.4854e+03, 2.7307e+03, 7.6665e+00, 7.4135e+00,\n",
      "        1.2573e+01, 1.2634e+01, 3.9510e+04, 1.6702e+03, 9.8801e+00, 4.1765e-01,\n",
      "        3.8570e-02, 2.1486e-02])\n",
      "17  :  tensor([1.0500e+02, 1.8600e+02, 1.5659e+03, 2.7854e+03, 7.5900e+00, 7.3734e+00,\n",
      "        1.2634e+01, 1.2897e+01, 3.9256e+04, 1.6725e+03, 9.8510e+00, 4.1970e-01,\n",
      "        3.6295e-02, 2.1349e-02])\n",
      "18  :  tensor([1.1700e+02, 2.3600e+02, 1.7426e+03, 3.5303e+03, 7.3999e+00, 7.3734e+00,\n",
      "        1.2732e+01, 1.2897e+01, 4.0017e+04, 1.5269e+03, 1.0019e+01, 3.8230e-01,\n",
      "        3.2752e-02, 1.6436e-02])\n",
      "19  :  tensor([2.1200e+02, 2.2300e+02, 1.2765e+03, 1.3431e+03, 4.5587e+00, 5.2699e+00,\n",
      "        1.5270e+01, 1.4177e+01, 9.7670e+04, 6.6105e+03, 9.8478e+00, 6.6658e-01,\n",
      "        4.5090e-02, 4.3534e-02])\n",
      "20  :  tensor([1.8500e+02, 2.3500e+02, 1.1137e+03, 1.4163e+03, 4.9822e+00, 4.6153e+00,\n",
      "        1.4677e+01, 1.5270e+01, 9.7258e+04, 4.8089e+03, 9.8112e+00, 4.8512e-01,\n",
      "        5.2222e-02, 4.1872e-02])\n",
      "21  :  tensor([1.7500e+02, 2.2800e+02, 1.0518e+03, 1.3722e+03, 5.7093e+00, 4.6580e+00,\n",
      "        1.4703e+01, 1.4332e+01, 9.7864e+04, 5.2703e+03, 9.8594e+00, 5.3096e-01,\n",
      "        5.6234e-02, 4.2991e-02])\n",
      "22  :  tensor([1.5000e+02, 1.5800e+02, 1.1218e+03, 1.1821e+03, 5.1642e+00, 5.6027e+00,\n",
      "        1.8266e+01, 1.7424e+01, 7.9667e+04, 4.9283e+03, 9.9971e+00, 6.1844e-01,\n",
      "        5.2493e-02, 5.0032e-02])\n",
      "23  :  tensor([1.1300e+02, 1.5800e+02, 8.3728e+02, 1.1737e+03, 5.2496e+00, 5.4074e+00,\n",
      "        1.8137e+01, 1.8044e+01, 7.9789e+04, 5.1533e+03, 9.9413e+00, 6.4207e-01,\n",
      "        6.9628e-02, 4.9804e-02])\n",
      "24  :  tensor([1.1800e+02, 1.6600e+02, 8.8036e+02, 1.2415e+03, 5.6317e+00, 5.4587e+00,\n",
      "        1.6949e+01, 1.8044e+01, 8.0021e+04, 4.3177e+03, 1.0035e+01, 5.4148e-01,\n",
      "        6.6992e-02, 4.7157e-02])\n",
      "25  :  tensor([1.2600e+02, 1.7900e+02, 9.2994e+02, 1.3313e+03, 5.4074e+00, 5.1852e+00,\n",
      "        1.6481e+01, 1.8266e+01, 8.1139e+04, 4.3995e+03, 1.0061e+01, 5.4842e-01,\n",
      "        6.2127e-02, 4.4223e-02])\n",
      "26  :  tensor([1.1300e+02, 1.6800e+02, 8.4168e+02, 1.2550e+03, 5.6859e+00, 5.6317e+00,\n",
      "        1.6481e+01, 1.7784e+01, 7.9142e+04, 4.6400e+03, 9.9126e+00, 5.8116e-01,\n",
      "        6.7947e-02, 4.7321e-02])\n",
      "27  :  tensor([1.2300e+02, 1.8100e+02, 9.1363e+02, 1.3408e+03, 5.5041e+00, 5.4898e+00,\n",
      "        1.6040e+01, 1.5717e+01, 8.0399e+04, 3.9190e+03, 1.0035e+01, 4.8652e-01,\n",
      "        6.4358e-02, 4.3923e-02])\n",
      "28  :  tensor([1.1600e+02, 1.7100e+02, 8.6282e+02, 1.2755e+03, 5.2963e+00, 6.0917e+00,\n",
      "        1.6333e+01, 1.8137e+01, 7.9948e+04, 4.6115e+03, 9.9972e+00, 5.7665e-01,\n",
      "        6.7879e-02, 4.5579e-02])\n",
      "29  :  tensor([1.1400e+02, 1.6100e+02, 8.4318e+02, 1.1939e+03, 5.4465e+00, 5.4074e+00,\n",
      "        1.6369e+01, 1.6881e+01, 8.0449e+04, 5.0839e+03, 1.0005e+01, 6.3224e-01,\n",
      "        6.9079e-02, 4.9161e-02])\n",
      "30  :  tensor([1.1700e+02, 1.8300e+02, 8.6427e+02, 1.3560e+03, 5.9509e+00, 5.5904e+00,\n",
      "        1.8249e+01, 1.7925e+01, 8.1326e+04, 4.5938e+03, 1.0099e+01, 5.7045e-01,\n",
      "        6.6915e-02, 4.3443e-02])\n",
      "31  :  tensor([1.1400e+02, 1.6900e+02, 8.4560e+02, 1.2572e+03, 5.6317e+00, 5.6027e+00,\n",
      "        1.5787e+01, 1.8195e+01, 8.1612e+04, 5.5243e+03, 1.0179e+01, 6.8898e-01,\n",
      "        6.8658e-02, 4.6775e-02])\n",
      "32  :  tensor([1.2600e+02, 1.6000e+02, 9.3064e+02, 1.1959e+03, 5.6859e+00, 5.6027e+00,\n",
      "        1.8044e+01, 1.6369e+01, 8.1464e+04, 5.3412e+03, 1.0108e+01, 6.6958e-01,\n",
      "        6.1468e-02, 4.8925e-02])\n",
      "33  :  tensor([1.0800e+02, 1.8100e+02, 8.0381e+02, 1.3593e+03, 5.9074e+00, 5.5204e+00,\n",
      "        1.6949e+01, 1.7784e+01, 8.0133e+04, 4.3762e+03, 1.0033e+01, 5.5081e-01,\n",
      "        7.0954e-02, 4.3917e-02])\n",
      "34  :  tensor([1.1300e+02, 1.7200e+02, 8.3582e+02, 1.2761e+03, 5.1642e+00, 5.1852e+00,\n",
      "        1.8137e+01, 1.6328e+01, 7.9789e+04, 4.4514e+03, 9.9240e+00, 5.5366e-01,\n",
      "        6.9274e-02, 4.5512e-02])\n",
      "35  :  tensor([1.2800e+02, 1.9000e+02, 9.5191e+02, 1.4166e+03, 5.6027e+00, 5.2496e+00,\n",
      "        1.8195e+01, 1.7925e+01, 8.0390e+04, 3.9545e+03, 1.0042e+01, 4.9400e-01,\n",
      "        6.1523e-02, 4.1842e-02])\n",
      "36  :  tensor([1.2200e+02, 1.6500e+02, 9.0501e+02, 1.2266e+03, 5.6859e+00, 5.5204e+00,\n",
      "        1.6328e+01, 1.7268e+01, 8.1222e+04, 4.7081e+03, 1.0125e+01, 5.8690e-01,\n",
      "        6.4164e-02, 4.7727e-02])\n",
      "37  :  tensor([1.2700e+02, 1.6500e+02, 9.4406e+02, 1.2288e+03, 5.2496e+00, 5.1642e+00,\n",
      "        1.8137e+01, 1.8044e+01, 7.9877e+04, 4.7087e+03, 9.9746e+00, 5.8800e-01,\n",
      "        6.1094e-02, 4.7970e-02])\n",
      "38  :  tensor([1.1700e+02, 1.7900e+02, 8.7569e+02, 1.3308e+03, 5.8531e+00, 5.2963e+00,\n",
      "        1.8044e+01, 1.8249e+01, 7.9210e+04, 4.9406e+03, 9.9660e+00, 6.1565e-01,\n",
      "        6.6906e-02, 4.4413e-02])\n",
      "39  :  tensor([1.2300e+02, 1.7300e+02, 9.1045e+02, 1.2836e+03, 5.8796e+00, 5.6027e+00,\n",
      "        1.7784e+01, 1.7626e+01, 8.1220e+04, 5.0495e+03, 1.0102e+01, 6.2805e-01,\n",
      "        6.2992e-02, 4.5699e-02])\n",
      "40  :  tensor([1.0900e+02, 1.6300e+02, 8.0788e+02, 1.2118e+03, 5.6828e+00, 5.2496e+00,\n",
      "        1.5420e+01, 1.6949e+01, 8.0729e+04, 5.4600e+03, 1.0065e+01, 6.8071e-01,\n",
      "        7.1505e-02, 4.8067e-02])\n",
      "41  :  tensor([1.0800e+02, 1.5800e+02, 7.9861e+02, 1.1718e+03, 5.5204e+00, 5.2963e+00,\n",
      "        1.6949e+01, 1.7925e+01, 8.0259e+04, 5.0196e+03, 9.9837e+00, 6.2441e-01,\n",
      "        7.2926e-02, 5.0095e-02])\n",
      "42  :  tensor([1.2000e+02, 1.7400e+02, 8.9239e+02, 1.2973e+03, 5.8531e+00, 5.4074e+00,\n",
      "        1.6109e+01, 1.7626e+01, 7.9703e+04, 4.2178e+03, 9.9616e+00, 5.2716e-01,\n",
      "        6.5625e-02, 4.5431e-02])\n",
      "43  :  tensor([1.0700e+02, 1.6100e+02, 7.8664e+02, 1.1874e+03, 5.7482e+00, 5.1642e+00,\n",
      "        1.5722e+01, 1.7925e+01, 8.1363e+04, 5.5021e+03, 1.0064e+01, 6.8053e-01,\n",
      "        7.3168e-02, 4.9099e-02])\n",
      "44  :  tensor([1.1000e+02, 1.8000e+02, 8.1577e+02, 1.3397e+03, 5.4465e+00, 5.6027e+00,\n",
      "        1.8195e+01, 1.8249e+01, 8.0350e+04, 4.7794e+03, 1.0022e+01, 5.9616e-01,\n",
      "        7.1164e-02, 4.3917e-02])\n",
      "45  :  tensor([1.2600e+02, 1.7800e+02, 9.3645e+02, 1.3260e+03, 6.0074e+00, 5.5041e+00,\n",
      "        1.7268e+01, 1.7784e+01, 8.0749e+04, 4.4672e+03, 1.0082e+01, 5.5777e-01,\n",
      "        6.1587e-02, 4.3978e-02])\n",
      "46  :  tensor([1.0100e+02, 1.5100e+02, 7.4748e+02, 1.1212e+03, 5.5041e+00, 5.7477e+00,\n",
      "        1.7424e+01, 1.8137e+01, 8.0150e+04, 4.9314e+03, 9.9850e+00, 6.1435e-01,\n",
      "        7.8703e-02, 5.2642e-02])\n",
      "47  :  tensor([1.1800e+02, 1.6400e+02, 8.7816e+02, 1.2234e+03, 5.6027e+00, 5.7477e+00,\n",
      "        1.8249e+01, 1.8266e+01, 8.0239e+04, 5.5423e+03, 1.0037e+01, 6.9331e-01,\n",
      "        6.6737e-02, 4.8201e-02])\n",
      "48  :  tensor([1.1200e+02, 1.6800e+02, 8.3053e+02, 1.2495e+03, 5.2963e+00, 5.1852e+00,\n",
      "        1.5356e+01, 1.7268e+01, 8.1896e+04, 4.4945e+03, 1.0213e+01, 5.6049e-01,\n",
      "        6.9893e-02, 4.6595e-02])\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for a in range(0, 49):\n",
    "    i=r(0, (len(contents)/15-1))\n",
    "    tt=torch.tensor(contents[(a*15):(a*15+14)])\n",
    "    print(a, ' : ', tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=14, out_features=14, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=14, out_features=4, bias=True)\n",
    "        #self.out = nn.Linear(in_features=14, out_features = 4, bias=True)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        #1st layer\n",
    "        t = self.fc1(t)\n",
    "        #2nd layer\n",
    "        t = self.fc2(t)\n",
    "        #output layer\n",
    "        #t = self.out(t)\n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "net=Network()\n",
    "params=list(net.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "net.zero_grad()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0000000005)\n",
    "lossDuplicationCounter = 0\n",
    "prevloss=0\n",
    "loss = 4\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:  0 \n",
      "loss:  tensor(66751988., grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-3089.6123,  5282.0879,  1153.5486, 15108.3203],\n",
      "       grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  1000 \n",
      "loss:  tensor(1201.7990, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 11.5529,  -1.5877, -31.5833, -59.6111], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  2000 \n",
      "loss:  tensor(2071.6899, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 60.7431, -42.8767, -42.3128, -30.1167], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  3000 \n",
      "loss:  tensor(620.5328, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-33.3064,  30.4814,   9.9914, -17.5437], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  4000 \n",
      "loss:  tensor(909.8060, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([  3.6790, -15.6043,   9.0378,  58.4501], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  5000 \n",
      "loss:  tensor(2756.3538, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 11.5702, -20.4334,  16.5932, 101.9885], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  6000 \n",
      "loss:  tensor(301.1143, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-15.5820,  15.3127,   1.4287, -25.9284], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  7000 \n",
      "loss:  tensor(364.1389, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-22.2205,  16.7441,   0.3679, -25.1210], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  8000 \n",
      "loss:  tensor(3104.0840, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-20.5561, -12.6929,  20.8063, 107.7697], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  9000 \n",
      "loss:  tensor(836.8842, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([  4.6691,  46.3704, -13.9598, -30.3153], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  10000 \n",
      "loss:  tensor(62.3091, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 9.1234,  9.4895, -4.7254, -6.3226], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  11000 \n",
      "loss:  tensor(791.3986, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 16.8506,   4.5906, -12.7813, -50.9347], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  12000 \n",
      "loss:  tensor(1396.7882, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-3.9653, -8.5415, 14.7590, 73.6680], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  13000 \n",
      "loss:  tensor(902.0688, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ -1.5779, -10.7859,  10.8500,  59.0666], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  14000 \n",
      "loss:  tensor(32.6036, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-3.6251,  3.1028,  2.5585, 11.0548], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  15000 \n",
      "loss:  tensor(110.8912, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-2.1122, -5.5154,  3.8732, 20.8414], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  16000 \n",
      "loss:  tensor(114.7765, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-2.1388, -5.5266,  3.4549, 21.2991], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  17000 \n",
      "loss:  tensor(113.7389, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([  0.3837,  -3.4330,  -4.0342, -20.4666], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  18000 \n",
      "loss:  tensor(8.2223, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 1.9433,  3.5068, -0.2291, -3.0942], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  19000 \n",
      "loss:  tensor(196.4927, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-3.1203, -1.9579,  6.1118, 28.1118], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  20000 \n",
      "loss:  tensor(23.1442, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 1.3609,  1.0377, -2.2833, -8.1888], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  21000 \n",
      "loss:  tensor(212.1683, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([  2.9142,  10.3170,  -8.0916, -24.8508], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  22000 \n",
      "loss:  tensor(11.4020, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([0.3859, 5.6698, 1.9550, 4.0806], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  23000 \n",
      "loss:  tensor(18.4885, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([0.0726, 3.7007, 1.6284, 8.5896], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  24000 \n",
      "loss:  tensor(4.7288, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 1.0020,  0.0308, -1.3878, -2.9980], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  25000 \n",
      "loss:  tensor(156.6763, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-3.4440, -0.9052,  5.9144, 25.0634], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  26000 \n",
      "loss:  tensor(91.5752, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-2.0248,  6.0245,  4.5612, 18.4672], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  27000 \n",
      "loss:  tensor(685.6665, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ -7.8578, -33.8894,   4.9308,  39.8345], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  28000 \n",
      "loss:  tensor(6.1324, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 0.0070,  1.3870, -2.5491, -3.0134], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  29000 \n",
      "loss:  tensor(92.5989, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([  1.7722, -11.3113,  -3.5755, -14.0508], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  30000 \n",
      "loss:  tensor(67.9136, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ -2.1620, -10.5930,   3.0424,  13.0629], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  31000 \n",
      "loss:  tensor(18.8441, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 0.6242, -2.0081, -2.2127, -7.1276], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  32000 \n",
      "loss:  tensor(607.5835, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-0.1218, 45.9104,  8.8962, 16.1309], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  33000 \n",
      "loss:  tensor(57.1022, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([  1.4682, -11.6855,  -2.2132,  -8.2089], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  34000 \n",
      "loss:  tensor(29.3030, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ -1.2984, -10.7224,   0.7448,   0.9645], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  35000 \n",
      "loss:  tensor(4.0038, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-0.1041,  3.6699,  0.6459,  2.4556], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  36000 \n",
      "loss:  tensor(3.2775, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 0.1271,  3.5112, -0.2979,  1.8223], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  37000 \n",
      "loss:  tensor(33.1796, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-1.6405, -0.4331,  3.1070, 11.9630], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  38000 \n",
      "loss:  tensor(30.5782, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 1.4595, 10.8320,  1.3071,  2.0682], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  39000 \n",
      "loss:  tensor(1.7732, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 0.4840,  2.4916, -0.5381,  0.3992], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  40000 \n",
      "loss:  tensor(30.8906, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-1.2269,  3.8426,  3.0694, 10.8929], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  41000 \n",
      "loss:  tensor(6.7229, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-1.1096, -4.8173, -0.6961, -0.4036], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  42000 \n",
      "loss:  tensor(16.5380, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-1.3037, -0.6812,  2.5461,  8.5833], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  43000 \n",
      "loss:  tensor(2.6538, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-0.0283, -2.2206, -0.9661, -1.1794], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  44000 \n",
      "loss:  tensor(31.4181, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 2.3216, 10.8538,  0.5019, -0.4921], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  45000 \n",
      "loss:  tensor(2.8058, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 0.8118,  1.6351, -0.7232, -1.7143], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  46000 \n",
      "loss:  tensor(29.4732, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([  1.7109, -10.4272,   2.4942,   1.1309], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  47000 \n",
      "loss:  tensor(124.8156, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-0.8776, 22.1340,  3.3112,  1.7986], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  48000 \n",
      "loss:  tensor(0.4168, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-0.2958,  0.5332, -1.1351,  1.0836], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  49000 \n",
      "loss:  tensor(1.3608, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 0.4007, -0.5074,  0.9771,  3.0175], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  50000 \n",
      "loss:  tensor(0.2366, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 0.0240,  0.8854, -0.3655,  1.1679], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  51000 \n",
      "loss:  tensor(20.8973, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-1.1547, -8.7778,  0.0851,  3.2802], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  52000 \n",
      "loss:  tensor(8.0190, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 0.0509,  5.6260, -0.0528,  0.3526], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  53000 \n",
      "loss:  tensor(116.1427, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ -0.6270, -21.4988,  -1.0657,   0.0809], grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "iteration:  54000 \n",
      "loss:  tensor(15.5345, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 0.1688, -7.5334,  0.0632, -1.3136], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  55000 \n",
      "loss:  tensor(0.9156, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 0.4138,  1.4541, -0.1927, -0.1574], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  56000 \n",
      "loss:  tensor(1.6397, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 0.6006,  1.3525,  0.0463, -1.0897], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  57000 \n",
      "loss:  tensor(2.8776, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-0.8665,  2.1111,  0.8245,  3.3713], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  58000 \n",
      "loss:  tensor(0.8001, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-0.6006,  1.4293, -0.1363,  0.1179], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  59000 \n",
      "loss:  tensor(8.6288, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-0.6073,  5.0640,  1.0633,  3.7152], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  60000 \n",
      "loss:  tensor(0.1988, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([ 0.4761,  0.6885, -0.1180,  0.7162], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  61000 \n",
      "loss:  tensor(2.9294, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-0.2380, -3.2434, -0.1805, -0.0530], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  62000 \n",
      "loss:  tensor(3.3825, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-0.1973,  3.6286, -0.4525,  0.6538], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  63000 \n",
      "loss:  tensor(1.3839, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-0.4520,  2.1578,  0.1406,  0.1906], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  64000 \n",
      "loss:  tensor(14.1741, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([0.8251, 7.4358, 0.5253, 0.3303], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  65000 \n",
      "loss:  tensor(22.9371, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([1.3099, 9.3926, 1.3132, 1.2957], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  66000 \n",
      "loss:  tensor(5.9493, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-0.6542, -4.5441,  0.1424,  2.6433], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  67000 \n",
      "loss:  tensor(0.9933, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-1.3641, -0.0499, -1.0557,  0.0023], grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "iteration:  68000 \n",
      "loss:  tensor(2037.2853, grad_fn=<MseLossBackward>) \n",
      "\tOutput:\n",
      " tensor([-43.0515, -71.0102, -27.3365, -21.4938], grad_fn=<AddBackward0>)\n",
      "Learning time: \n",
      "\t 17.802095651626587\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "while ((loss > 0.001) and (lossDuplicationCounter<10)):\n",
    "    a=r(0, (len(contents)/15-1))\n",
    "    chars = torch.tensor(contents[(a*15):(a*15+14)])\n",
    "    optimizer.zero_grad()   # zero the gradient buffers\n",
    "    out = net(chars)\n",
    "    if (contents[a*14+15]=='Marcin'):\n",
    "        target = torch.tensor([1., 0., 0., 0.])\n",
    "    elif (contents[a*14+15]=='Bartek'):\n",
    "        target = torch.tensor([0., 1., 0., 0.])\n",
    "    elif (contents[a*14+15]=='Maciek'):\n",
    "        target = torch.tensor([0., 0., 1., 0.,])\n",
    "    else:\n",
    "        target = torch.tensor([0., 0., 0., 1.,])\n",
    "    loss = criterion(out, target)\n",
    "    if prevloss == loss:\n",
    "        lossDuplicationCounter +=1\n",
    "        print('loss duplicate #', lossDuplicationCounter)\n",
    "    else:\n",
    "        lossDuplicationCounter = 0\n",
    "    prevloss = loss\n",
    "    loss.backward(retain_graph=True)\n",
    "    #loss.backward()\n",
    "    optimizer.step()    # Does the update\n",
    "    if i%1000==0:\n",
    "        print(\"\\n\\niteration: \", i, \"\\nloss: \", loss, '\\n\\tOutput:\\n', out)\n",
    "    i+=1\n",
    "print(\"Learning time: \\n\\t\", time.time()-start)\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -0.8419, -29.8164,  -0.9564,   4.0439] \t :  Maciek\n",
      "[ 0.8882, 34.7605,  5.8932,  4.3201] \t :  Maciek\n",
      "[ 0.3619,  2.4204, -0.5053, -0.4203] \t :  Maciek\n",
      "[-0.2969, -5.6377,  0.5202,  0.6555] \t :  Marcin\n",
      "[ 0.1018, -0.3584,  0.8493,  0.4865] \t :  Marcin\n",
      "[0.0484, 1.4585, 1.2443, 0.7185] \t :  Marcin\n",
      "[-0.3481, -5.1987,  0.8425,  1.0701] \t :  Marcin\n",
      "[-0.5607, -8.3938, -0.2576,  1.8536] \t :  Bartek\n",
      "[-0.1673,  6.1702,  1.0975,  2.5996] \t :  Bartek\n",
      "[-0.3759, -2.5265,  0.0269,  1.9440] \t :  Bartek\n",
      "[-0.2936, -1.4823, -0.2322,  1.8550] \t :  Bartek\n",
      "[-0.3619, -2.8664, -0.1761,  1.8569] \t :  Bartek\n",
      "[-0.2879, -3.9325, -0.3127,  1.9177] \t :  Bartek\n",
      "[-0.4021, -7.1102,  0.0248,  2.1638] \t :  Bartek\n",
      "[-0.2234,  0.3945,  0.0435,  2.1155] \t :  Bartek\n",
      "[-0.1805,  2.5387,  0.4962,  2.3592] \t :  Bartek\n",
      "[-0.1631,  3.6906,  0.2181,  2.1375] \t :  Bartek\n",
      "[-0.1565,  2.9754,  0.1786,  2.1769] \t :  Bartek\n",
      "[-0.1308,  4.6806,  0.7763,  2.5353] \t :  Bartek\n",
      "[ -0.5475, -11.3230,   0.6765,   0.3956] \t :  Marcin\n",
      "[-0.1479, -7.1135,  0.7849,  0.0990] \t :  Marcin\n",
      "[-0.2793, -5.7195,  1.0158, -0.0074] \t :  Marcin\n",
      "[ 0.0751, -4.5587, -0.8272,  1.2978] \t :  Artur\n",
      "[ 0.1428,  0.7658, -0.1583,  1.0501] \t :  Artur\n",
      "[ 0.2255,  0.4857, -0.1788,  1.0339] \t :  Artur\n",
      "[ 0.2153, -0.4569,  0.0629,  0.9747] \t :  Artur\n",
      "[0.1061, 0.8766, 0.1349, 0.8957] \t :  Artur\n",
      "[ 0.0582, -0.0192,  0.1501,  0.6071] \t :  Artur\n",
      "[0.0916, 0.6939, 0.1269, 0.9048] \t :  Artur\n",
      "[0.0507, 0.8128, 0.2092, 0.9186] \t :  Artur\n",
      "[0.0837, 0.7687, 0.0768, 0.7215] \t :  Artur\n",
      "[0.0082, 0.9292, 0.5820, 0.9451] \t :  Artur\n",
      "[-0.0421, -0.7167, -0.1717,  0.9818] \t :  Artur\n",
      "[0.1266, 2.0381, 0.3161, 0.7134] \t :  Artur\n",
      "[ 0.1157,  1.0894, -0.0508,  0.6995] \t :  Artur\n",
      "[ 0.1971, -0.7483, -0.1705,  0.7649] \t :  Artur\n",
      "[ 0.1048, -0.0058,  0.0195,  0.9938] \t :  Artur\n",
      "[ 0.2042, -1.0779, -0.3878,  1.1055] \t :  Artur\n",
      "[0.0629, 0.0587, 0.1638, 0.7786] \t :  Artur\n",
      "[ 0.0178, -0.3253,  0.0353,  0.8763] \t :  Artur\n",
      "[-0.0071,  1.4541,  0.6391,  0.8687] \t :  Artur\n",
      "[0.1660, 1.6447, 0.0837, 1.0245] \t :  Artur\n",
      "[0.1603, 0.1333, 0.0739, 0.8887] \t :  Artur\n",
      "[0.0807, 1.8001, 0.5554, 0.9796] \t :  Artur\n",
      "[0.1025, 1.5465, 0.1823, 0.7356] \t :  Artur\n",
      "[ 0.1039, -0.6133, -0.0413,  0.8806] \t :  Artur\n",
      "[ 0.1860,  2.7172, -0.0914,  1.0606] \t :  Artur\n",
      "[ 0.0049,  0.0056, -0.0010,  0.9983] \t :  Artur\n",
      "[0.2053, 1.7438, 0.3378, 0.9309] \t :  Artur\n",
      "[0.2274, 0.8882, 0.1020, 0.8290] \t :  Artur\n",
      "[ 0.1169, -0.6227, -0.1119,  0.7427] \t :  Artur\n",
      "[ 0.0261,  1.8475, -0.0652,  0.6284] \t :  Artur\n",
      "[-0.0012,  0.6344,  0.2746,  0.9206] \t :  Artur\n",
      "[0.1140, 1.4584, 0.0148, 1.0734] \t :  Artur\n",
      "[ 0.1509,  0.9982, -0.2349,  1.0107] \t :  Artur\n",
      "[ 0.2092,  2.2042, -0.0633,  0.8573] \t :  Artur\n",
      "[0.2333, 0.6278, 0.0016, 0.7503] \t :  Artur\n",
      "[0.3284, 2.4978, 0.0676, 0.8851] \t :  Artur\n",
      "[ 0.1391, -0.0224,  0.4599,  0.5745] \t :  Artur\n",
      "[ 0.0972, -0.1536,  0.0112,  0.7625] \t :  Artur\n",
      "[ -0.4127, -10.6243,   0.3928,   0.8055] \t :  Marcin\n",
      "[-0.0834, -9.9905, -0.3904,  0.7564] \t :  Marcin\n",
      "[ 0.0722, -9.0945, -0.3338,  0.8712] \t :  Marcin\n",
      "[ -0.0606, -12.5276,  -0.4324,   0.6044] \t :  Marcin\n",
      "[ -0.4326, -13.1501,  -0.1951,   1.0702] \t :  Marcin\n",
      "[ -0.1283, -11.3545,  -0.1966,   0.9697] \t :  Marcin\n",
      "[-0.2864, -9.7717,  0.4279,  0.4283] \t :  Marcin\n",
      "[ -0.2379, -11.2917,   0.1027,   0.8945] \t :  Marcin\n",
      "[ -0.4852, -20.9932,  -0.5569,   0.9275] \t :  Marcin\n",
      "[ -0.6696, -16.9804,  -1.2922,   1.4850] \t :  Bartek\n",
      "[-0.3281, -3.5812, -0.1191,  2.1124] \t :  Bartek\n",
      "[-0.3434, -1.4350,  0.8443,  2.6650] \t :  Bartek\n",
      "[-0.9460, -9.3804,  0.7972,  2.0252] \t :  Bartek\n",
      "[-0.4965, -6.8375,  0.7051,  2.5157] \t :  Bartek\n",
      "[-0.2281,  2.2384,  0.4836,  2.4171] \t :  Bartek\n",
      "[-0.2019,  3.7290,  0.9776,  2.6458] \t :  Bartek\n",
      "[-0.2857, -2.1986,  0.1480,  2.3573] \t :  Bartek\n",
      "[-0.1922,  2.2908,  0.3580,  2.3347] \t :  Bartek\n",
      "[-0.2369,  0.7733,  0.2734,  2.3036] \t :  Bartek\n",
      "[-0.3845, -3.8007,  0.2734,  2.3547] \t :  Bartek\n",
      "[-0.2340,  1.2573,  0.5701,  2.4488] \t :  Bartek\n",
      "[-0.3868, -4.4221,  0.0851,  2.2649] \t :  Bartek\n",
      "[-0.3116,  0.0617,  0.5385,  2.4256] \t :  Bartek\n",
      "[-0.2921,  0.1790,  0.5661,  2.5210] \t :  Bartek\n",
      "[-0.3589, -3.7774,  0.3843,  2.4460] \t :  Bartek\n",
      "[-0.1356,  5.0057,  1.0403,  2.8315] \t :  Bartek\n",
      "[-0.2860, -0.4087,  1.0305,  2.8525] \t :  Bartek\n",
      "[ -0.5444, -11.6316,   0.0358,   2.2660] \t :  Bartek\n",
      "[-0.3923, -7.1738, -0.3661,  2.0586] \t :  Bartek\n",
      "[-0.3950, -5.5065,  0.2822,  2.4403] \t :  Bartek\n",
      "[-0.2233,  1.2156,  0.7187,  2.6117] \t :  Bartek\n",
      "[-0.1525, -3.4061,  0.2626,  0.7223] \t :  Maciek\n",
      "[-0.2940, -2.5190,  0.8859,  0.3170] \t :  Maciek\n",
      "[-0.1077, -5.1586, -0.1004,  0.8644] \t :  Maciek\n",
      "[-0.0954, -2.7250,  0.2582,  0.5198] \t :  Maciek\n",
      "[0.0984, 1.8594, 0.3207, 0.5361] \t :  Maciek\n",
      "[-0.1813, -0.1379,  0.5383,  0.8848] \t :  Maciek\n",
      "[ 0.1135, -1.4437,  0.0226,  0.6018] \t :  Maciek\n",
      "[ 0.1502, -1.1548, -0.2005,  0.5255] \t :  Maciek\n",
      "[ 0.2451, -0.5350, -0.2986,  0.6647] \t :  Maciek\n",
      "[ 0.0239, -3.3364,  0.0910,  0.4330] \t :  Maciek\n",
      "[-0.0104,  0.3262,  0.5131,  0.3626] \t :  Maciek\n",
      "[-0.0115, -1.7645,  0.1464,  0.7164] \t :  Maciek\n",
      "[-0.3943, -3.2467,  0.6288,  0.8830] \t :  Maciek\n",
      "[-0.0438, -1.7622,  0.3369,  0.5486] \t :  Maciek\n",
      "[-0.0162, -1.0881,  0.1957,  0.6510] \t :  Maciek\n",
      "[ 0.1510, -2.5660, -0.2366,  0.7356] \t :  Maciek\n",
      "[ 0.0304, -2.0793,  0.1030,  0.4609] \t :  Maciek\n",
      "[-0.3336, -7.5983, -0.0274,  1.0678] \t :  Maciek\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "for filename in glob.glob(path):\n",
    "    #a=r(0, (len(contents)/15-1))\n",
    "    chars = torch.tensor(contents[(i*15):(i*15+14)])\n",
    "    print(str(net(chars)).strip(\", grad_fn=<AddBackward0>)\").strip(\"tensor(\"), \"\\t : \",contents[i*15+14])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "fc1.weight \t torch.Size([14, 14])\n",
      "fc1.bias \t torch.Size([14])\n",
      "fc2.weight \t torch.Size([4, 14])\n",
      "fc2.bias \t torch.Size([4])\n",
      "Optimizer's state_dict:\n",
      "state \t {}\n",
      "param_groups \t [{'lr': 5e-10, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [2570222566064, 2570222567144, 2570222566712, 2570222565848]}]\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in net.state_dict():\n",
    "    print(param_tensor, \"\\t\", net.state_dict()[param_tensor].size())\n",
    "\n",
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net, \"net.dat\")\n",
    "torch.save(net.state_dict(), \"netstate.dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haba = torch.load(\"net.dat\")\n",
    "baba = Network()\n",
    "baba.load_state_dict(torch.load(\"netstate.dat\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
